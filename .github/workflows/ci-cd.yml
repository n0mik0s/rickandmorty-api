name: CI/CD Pipeline

on:
  push:
    branches: [main, master, 'feature_*']
  pull_request:
    branches: [main, master]

env:
  REGISTRY: docker.io
  IMAGE_NAME: ${{ secrets.DOCKERHUB_USERNAME }}/rickandmorty-api
  APP_DIR: .
  # Kubernetes namespace used in every kubectl/helm command
  K8S_NAMESPACE: rickandmorty

jobs:
  # ─────────────────────────────────────────────
  # 1. Code Quality
  # ─────────────────────────────────────────────
  lint-and-security:
    name: Lint & Security Scan
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install

      - name: Install dependencies
        working-directory: ${{ env.APP_DIR }}
        run: uv sync --all-extras

      # Ruff covers both linting and formatting checks
      - name: Lint with Ruff
        working-directory: ${{ env.APP_DIR }}
        run: uv run ruff check . --output-format=github

      - name: Format check with Ruff
        working-directory: ${{ env.APP_DIR }}
        run: uv run ruff format --check .

      # Static type checking
      - name: Type check with mypy
        working-directory: ${{ env.APP_DIR }}
        run: uv run mypy main.py --ignore-missing-imports
        continue-on-error: true   # advisory; flip to false to enforce

      # Security: dependency vulnerabilities
      - name: Security scan with pip-audit
        working-directory: ${{ env.APP_DIR }}
        run: uv run pip-audit

      # Security: source-code patterns (secrets, hardcoded creds, etc.)
      - name: Security scan with Bandit
        working-directory: ${{ env.APP_DIR }}
        run: uv run bandit -r main.py -c pyproject.toml -ll

  # ─────────────────────────────────────────────
  # 2. Unit Tests
  # ─────────────────────────────────────────────
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint-and-security

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Set up Python
        run: uv python install

      - name: Install dependencies
        working-directory: ${{ env.APP_DIR }}
        run: uv sync --all-extras

      # Provide dummy config/secret files so the module-level argparse
      # does not fail during import inside the test suite.
      - name: Create dummy config files
        run: |
          mkdir -p tmp
          echo '{"user":"test","password":"test","host":"localhost","dbname":"testdb"}' > tmp/secrets.json
          printf 'log_level: INFO\n' > tmp/config.yaml

      - name: Run unit tests
        working-directory: ${{ env.APP_DIR }}
        env:
          CONFIG_PATH: ../tmp/config.yaml
          SECRET_PATH: ../tmp/secrets.json
        run: |
          uv run pytest tests/unit/ \
            --tb=short \
            -v \
            --cov=main \
            --cov-report=xml \
            --cov-report=term-missing

      - name: Upload coverage report
        uses: codecov/codecov-action@v4
        if: always()
        with:
          files: ${{ env.APP_DIR }}/coverage.xml
          fail_ci_if_error: false

  # ─────────────────────────────────────────────
  # 3. Build & Test Docker Image
  # ─────────────────────────────────────────────
  build-and-test:
    name: Build & Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: testuser
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image (local, no push)
        uses: docker/build-push-action@v5
        with:
          context: ${{ env.APP_DIR }}
          push: false
          load: true
          tags: ${{ env.IMAGE_NAME }}:test
          cache-from: type=gha
          cache-to: type=gha,mode=max

      # ── Scan the built image for OS/package CVEs ──
      - name: Scan image with Trivy
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.IMAGE_NAME }}:test
          format: table
          exit-code: 1            # fail on CRITICAL vulnerabilities
          severity: CRITICAL
          ignore-unfixed: true

      # ── Spin up the app container for integration tests ──
      - name: Create runtime config files
        run: |
          mkdir -p tmp
          cat > tmp/secrets.json <<'EOF'
          {"user":"testuser","password":"testpass","host":"localhost","dbname":"rickandmorty"}
          EOF
          printf 'log_level: INFO\n' > tmp/config.yaml

      - name: Run application container
        run: |
          docker run -d \
            --name app \
            --network host \
            -v ${{ github.workspace }}/tmp:/tmp/appconfig:ro \
            ${{ env.IMAGE_NAME }}:test \
            --config /tmp/appconfig/config.yaml \
            --secret /tmp/appconfig/secrets.json

      - name: Wait for application to be ready
        run: |
          for i in $(seq 1 30); do
            if curl -sf http://localhost:8000/docs > /dev/null 2>&1; then
              echo "App is up!"
              break
            fi
            echo "Waiting... attempt $i"
            sleep 3
          done

      - name: Install uv (for integration test runner)
        uses: astral-sh/setup-uv@v4

      - name: Set up Python
        run: uv python install

      - name: Install test dependencies
        working-directory: ${{ env.APP_DIR }}
        run: uv sync --all-extras

      - name: Run integration tests
        working-directory: ${{ env.APP_DIR }}
        env:
          API_BASE_URL: http://localhost:8000
          DB_HOST: localhost
          DB_USER: testuser
          DB_PASSWORD: testpass
          DB_NAME: rickandmorty
        run: |
          uv run pytest tests/integration/ \
            --tb=short \
            -v

      - name: Dump container logs on failure
        if: failure()
        run: docker logs app

  # ─────────────────────────────────────────────
  # 4. Push to Docker Hub
  # ─────────────────────────────────────────────
  push-image:
    name: Push to Docker Hub
    runs-on: ubuntu-latest
    needs: build-and-test
    # Only push on commits to the default branch (not on PRs)
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')

    steps:
      #- name: Set current date as environment variable
      #  run: echo "NOW=$(date +'%Y-%m-%d')" >> $GITHUB_ENV

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Extract Docker metadata (tags & labels)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.IMAGE_NAME }}
          tags: type=raw,value=build-{{date 'YYYYMMDD-HHmm'}}-{{sha}}
          #tags: |
          #  type=sha,prefix=sha-
          #  type=ref,event=branch
          #  type=raw,value=build-${{ env.NOW }},enable=${{ github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' }}

      - name: Build and push image
        uses: docker/build-push-action@v5
        with:
          context: ${{ env.APP_DIR }}
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64,linux/arm64

# ─────────────────────────────────────────────────────────────────────────────
  # 5. Deploy to KinD + Monitoring Stack (merged to share runner)
  #
  # Two KinD clusters on the same runner:
  #   kind-rickandmorty — app + PostgreSQL + ESO + kube-prometheus-stack (no Grafana)
  #   kind-monitoring   — Prometheus (federates from app cluster) + Grafana + Alerta
  #
  # Federation path:
  #   App cluster Prometheus (NodePort 30909 on host) 
  #     → reachable at HOST_IP:30909 from inside monitoring cluster pods
  #     → monitoring Prometheus scrapes /federate endpoint
  #     → Grafana queries monitoring Prometheus
  #
  # Why HOST_IP and not localhost:
  #   KinD pods run inside Docker containers. From inside a pod, "localhost"
  #   is the pod itself. The host machine is reachable via the Docker bridge
  #   gateway IP (typically 172.17.0.1), discovered at runtime.
  # ─────────────────────────────────────────────────────────────────────────────
  deploy-kind:
    name: Deploy to KinD + Monitoring
    runs-on: ubuntu-latest
    needs: build-and-test
    if: >-
      github.event_name == 'push' && 
      (
        github.ref == 'refs/heads/main' || 
        github.ref == 'refs/heads/master' || 
        startsWith(github.ref, 'refs/heads/feature_')
      )

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # ── a) App KinD cluster ───────────────────────────────────────────────
      - name: Write app KinD cluster config
        run: |
          cat > /tmp/kind-config.yaml << 'EOF'
          kind: Cluster
          apiVersion: kind.x-k8s.io/v1alpha4
          nodes:
            - role: control-plane
              extraPortMappings:
                - containerPort: 30800
                  hostPort: 8000
                  protocol: TCP
                - containerPort: 30909
                  hostPort: 30909
                  protocol: TCP
                - containerPort: 30980
                  hostPort: 30980
                  protocol: TCP
          EOF

      - name: Create app KinD cluster
        uses: helm/kind-action@v1.10.0
        with:
          cluster_name: rickandmorty
          config: /tmp/kind-config.yaml

      # ── b) Build and load app image ───────────────────────────────────────
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build image for KinD (amd64 only, local load)
        uses: docker/build-push-action@v5
        with:
          context: ${{ env.APP_DIR }}
          push: false
          load: true
          tags: ${{ env.IMAGE_NAME }}:kind
          cache-from: type=gha

      - name: Load image into KinD
        run: kind load docker-image ${{ env.IMAGE_NAME }}:kind --name rickandmorty

      # ── c) cert-manager ───────────────────────────────────────────────────
      - name: Install cert-manager
        run: |
          helm repo add jetstack https://charts.jetstack.io --force-update
          helm upgrade --install cert-manager jetstack/cert-manager \
            --namespace cert-manager \
            --create-namespace \
            --set installCRDs=true \
            --wait \
            --timeout 3m

      # ── d) ESO + wait for CRDs ────────────────────────────────────────────
      - name: Create app namespace
        run: |
          kubectl config use-context kind-rickandmorty
          kubectl create namespace ${{ env.K8S_NAMESPACE }} \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Install External Secrets Operator
        run: |
          helm repo add external-secrets https://charts.external-secrets.io --force-update
          helm upgrade --install external-secrets external-secrets/external-secrets \
            --namespace ${{ env.K8S_NAMESPACE }} \
            --set installCRDs=true \
            --wait \
            --timeout 3m

      - name: Wait for ESO CRDs to be Established and discoverable
        run: |
          for crd in \
            externalsecrets.external-secrets.io \
            secretstores.external-secrets.io \
            clustersecretstores.external-secrets.io; do
            echo "Waiting for CRD object: $crd"
            kubectl wait --for=condition=Established crd/$crd --timeout=60s
          done

          echo "Polling until SecretStore appears in API discovery..."
          for i in $(seq 1 30); do
            if kubectl api-resources --api-group=external-secrets.io 2>/dev/null \
                | grep -q "secretstores.*v1"; then
              echo "SecretStore is discoverable after $i attempts."
              break
            fi
            echo "Attempt $i/30 — not yet discoverable, retrying in 5s..."
            sleep 5
          done

          if ! kubectl api-resources --api-group=external-secrets.io 2>/dev/null \
              | grep -q "secretstores.*v1"; then
            echo "ERROR: SecretStore never became discoverable."
            exit 1
          fi

      # ── e) Push raw secret ────────────────────────────────────────────────
      - name: Push db secret to cluster
        env:
          DB_SECRET_JSON: ${{ secrets.RICKANDMORTY_DB_SECRET }}
        run: |
          kubectl create secret generic rickandmorty-db-raw \
            --from-literal=secrets.json="${DB_SECRET_JSON}" \
            --namespace ${{ env.K8S_NAMESPACE }} \
            --dry-run=client -o yaml | kubectl apply -f -

      # ── f) Apply ESO resources ─────────────────────────────────────────────
      - name: Apply SecretStore
        run: |
          kubectl apply -f - <<'EOF'
          apiVersion: external-secrets.io/v1
          kind: SecretStore
          metadata:
            name: rickandmorty-secret-store
            namespace: rickandmorty
          spec:
            provider:
              kubernetes:
                remoteNamespace: rickandmorty
                server:
                  caProvider:
                    type: ConfigMap
                    name: kube-root-ca.crt
                    key: ca.crt
                auth:
                  serviceAccount:
                    name: external-secrets
                    namespace: rickandmorty
          EOF

      - name: Apply ExternalSecret
        run: |
          kubectl apply -f - <<'EOF'
          apiVersion: external-secrets.io/v1
          kind: ExternalSecret
          metadata:
            name: rickandmorty-api-db
            namespace: rickandmorty
          spec:
            refreshInterval: 1h
            secretStoreRef:
              kind: SecretStore
              name: rickandmorty-secret-store
            target:
              name: rickandmorty-db-secret
              creationPolicy: Owner
              template:
                type: Opaque
                data:
                  secrets.json: "{{ .secretsJson }}"
            data:
              - secretKey: secretsJson
                remoteRef:
                  key: rickandmorty-db-raw
                  property: secrets.json
          EOF

      # ── g) PostgreSQL ─────────────────────────────────────────────────────
      - name: Deploy PostgreSQL
        run: |
          helm repo add bitnami https://charts.bitnami.com/bitnami --force-update
          helm upgrade --install postgres bitnami/postgresql \
            --namespace ${{ env.K8S_NAMESPACE }} \
            --set auth.username=rickandmorty \
            --set auth.password=rickandmorty \
            --set auth.database=rickandmorty \
            --wait \
            --timeout 3m

      # ── h) Helm install app chart ─────────────────────────────────────────
      - name: Helm install rickandmorty-api
        run: |
          helm upgrade --install rickandmorty-api ./helm/rickandmorty-api \
            --namespace ${{ env.K8S_NAMESPACE }} \
            --set image.repository=${{ env.IMAGE_NAME }} \
            --set image.tag=kind \
            --set image.pullPolicy=Never \
            --set service.type=NodePort \
            --set service.nodePort=30800 \
            --set config.log_level=INFO \
            --set externalSecret.targetSecretName=rickandmorty-db-secret \
            --wait \
            --timeout 5m

      # ── i) Wait for ESO sync + rollout ────────────────────────────────────
      - name: Wait for ExternalSecret to sync
        run: |
          for i in $(seq 1 24); do
            STATUS=$(kubectl get externalsecret rickandmorty-api-db \
              -n ${{ env.K8S_NAMESPACE }} \
              -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' \
              2>/dev/null || echo "")
            if [ "$STATUS" = "True" ]; then
              echo "ExternalSecret synced."
              break
            fi
            echo "Attempt $i/24 — status: '${STATUS}'. Retrying in 5s..."
            sleep 5
          done
          kubectl get externalsecret -n ${{ env.K8S_NAMESPACE }}

      - name: Wait for Deployment rollout
        run: |
          kubectl rollout status deployment/rickandmorty-api \
            --namespace ${{ env.K8S_NAMESPACE }} \
            --timeout=3m

      # ── j) kube-prometheus-stack in APP cluster ───────────────────────────
      # Grafana is disabled — this cluster only collects metrics.
      # Prometheus NodePort 30909 is exposed on the host so the monitoring
      # cluster can reach it via the Docker bridge gateway IP.
      - name: Add Prometheus community Helm repo
        run: |
          helm repo add prometheus-community \
            https://prometheus-community.github.io/helm-charts --force-update
          helm repo update

      - name: Install kube-prometheus-stack in app cluster
        run: |
          kubectl config use-context kind-rickandmorty
          helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --create-namespace \
            --set prometheus.prometheusSpec.retention=7d \
            --set prometheus.service.type=NodePort \
            --set prometheus.service.nodePort=30909 \
            --set grafana.enabled=false \
            --set alertmanager.enabled=true \
            --set alertmanager.service.type=NodePort \
            --set alertmanager.service.nodePort=30980 \
            --wait \
            --timeout 5m

      - name: Apply ServiceMonitor for rickandmorty-api
        run: |
          kubectl config use-context kind-rickandmorty
          kubectl apply -f ./monitoring/prometheus/servicemonitor.yaml

      - name: Apply Prometheus alerting rules
        run: |
          kubectl config use-context kind-rickandmorty
          kubectl apply -f ./monitoring/prometheus/rules.yaml

      # ── k) Discover Docker bridge IP ──────────────────────────────────────
      # KinD pods cannot use "localhost" to reach the host — they are inside
      # Docker containers. The Docker bridge gateway is the host's IP as seen
      # from any KinD pod, and is used as the federation target.
      - name: Discover Docker bridge gateway IP
        run: |
          HOST_IP=$(docker network inspect kind \
            | jq -r '.[0].IPAM.Config[0].Gateway')
          echo "HOST_IP=$HOST_IP" >> $GITHUB_ENV
          echo "Docker bridge gateway (host IP for KinD pods): $HOST_IP"

      # ── l) Monitoring KinD cluster ────────────────────────────────────────
      - name: Write monitoring KinD cluster config
        run: |
          cat > /tmp/kind-config-monitoring.yaml << 'EOF'
          kind: Cluster
          apiVersion: kind.x-k8s.io/v1alpha4
          nodes:
            - role: control-plane
              extraPortMappings:
                - containerPort: 30300
                  hostPort: 3000
                  protocol: TCP
                - containerPort: 30400
                  hostPort: 4000
                  protocol: TCP
          EOF

      - name: Create monitoring KinD cluster
        uses: helm/kind-action@v1.10.0
        with:
          cluster_name: monitoring
          config: /tmp/kind-config-monitoring.yaml

      # ── m) kube-prometheus-stack in MONITORING cluster ────────────────────
      # Prometheus is ENABLED here and configured with additionalScrapeConfigs
      # to federate from the app cluster's Prometheus at HOST_IP:30909.
      # Only rickandmorty-api metrics are pulled across (match[] filter).
      - name: Install kube-prometheus-stack in monitoring cluster
        run: |
          kubectl config use-context kind-monitoring

          cat > /tmp/federation-scrape-config.yaml << EOF
          - job_name: federate-rickandmorty
            honor_labels: true
            metrics_path: /federate
            params:
              match[]:
                - '{job="rickandmorty-api"}'
                - '{__name__=~"rickandmorty_.*"}'
                - '{__name__=~"up|scrape_.*", job="rickandmorty-api"}'
            static_configs:
              - targets:
                  - "${{ env.HOST_IP }}:30909"
                labels:
                  source_cluster: rickandmorty
            scrape_interval: 30s
            scrape_timeout: 10s
          EOF

          helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --create-namespace \
            --set prometheus.enabled=true \
            --set prometheus.service.type=ClusterIP \
            --set grafana.enabled=true \
            --set grafana.service.type=NodePort \
            --set grafana.service.nodePort=30300 \
            --set grafana.adminPassword=admin \
            --set alertmanager.enabled=true \
            --set alertmanager.service.type=NodePort \
            --set alertmanager.service.nodePort=30980 \
            --set-json "prometheus.prometheusSpec.additionalScrapeConfigs=$(
              cat /tmp/federation-scrape-config.yaml \
                | python3 -c 'import sys,json,yaml; print(json.dumps(yaml.safe_load(sys.stdin.read())))'
            )" \
            --wait \
            --timeout 5m

      # ── n) Alerta ─────────────────────────────────────────────────────────
      - name: Install Alerta in monitoring cluster
        run: |
          kubectl config use-context kind-monitoring
          helm repo add alerta-web https://hayk96.github.io/alerta-web --force-update
          helm upgrade --install alerta alerta-web/alerta-web \
            --namespace monitoring \
            --set persistence.enabled=true \
            --set persistence.size=1Gi \
            --set service.type=NodePort \
            --set service.nodePort=30400 \
            --set alertaAuthRequired=false \
            --wait \
            --timeout 3m

      # ── o) Configure Alertmanager → Alerta ───────────────────────────────
      - name: Configure Alertmanager for Alerta
        run: |
          kubectl config use-context kind-monitoring
          kubectl wait --for=condition=Ready pod \
            -l app.kubernetes.io/name=alertmanager \
            -n monitoring \
            --timeout=120s || true
          kubectl apply -f ./monitoring/prometheus/alertmanager-config.yaml

      # ── p) Grafana dashboards ─────────────────────────────────────────────
      - name: Create Grafana dashboards ConfigMap
        run: |
          kubectl config use-context kind-monitoring

          kubectl create configmap rickandmorty-api-dashboard \
            --from-file=rickandmorty-api-dashboard.json=./monitoring/grafana/dashboards/rickandmorty-api-dashboard.json \
            --namespace monitoring \
            --dry-run=client -o yaml \
            | kubectl apply -f -
          kubectl label configmap rickandmorty-api-dashboard \
            grafana_dashboard=1 \
            --namespace monitoring \
            --overwrite

          kubectl create configmap rickandmorty-cluster-dashboard \
            --from-file=rickandmorty-cluster-dashboard.json=./monitoring/grafana/dashboards/rickandmorty-cluster-dashboard.json \
            --namespace monitoring \
            --dry-run=client -o yaml \
            | kubectl apply -f -
          kubectl label configmap rickandmorty-cluster-dashboard \
            grafana_dashboard=1 \
            --namespace monitoring \
            --overwrite

      # ── q) Verify federation is working ───────────────────────────────────
      - name: Wait for federation scrape to succeed
        run: |
          kubectl config use-context kind-monitoring
          echo "Waiting for monitoring Prometheus to be ready..."
          kubectl wait --for=condition=Ready pod \
            -l app.kubernetes.io/name=prometheus \
            -n monitoring \
            --timeout=120s

          echo "Polling federation target health..."
          # Port-forward monitoring Prometheus to query its targets API
          kubectl port-forward svc/prometheus-kube-prometheus-prometheus \
            9090:9090 -n monitoring &
          PF_PID=$!
          sleep 5

          for i in $(seq 1 12); do
            HEALTH=$(curl -s \
              "http://localhost:9090/api/v1/targets?job=federate-rickandmorty" \
              | jq -r '.data.activeTargets[0].health' 2>/dev/null || echo "unknown")
            echo "Attempt $i/12 — federation target health: $HEALTH"
            if [ "$HEALTH" = "up" ]; then
              echo "✅ Federation target is UP"
              kill $PF_PID 2>/dev/null || true
              break
            fi
            sleep 10
          done
          kill $PF_PID 2>/dev/null || true

      # ── r) Smoke tests ─────────────────────────────────────────────────────
      - name: Smoke test — /sync reachable
        run: |
          curl -sf "http://localhost:8000/sync/?source_url=rickandmortyapi.com&resource=character" \
            > /dev/null && echo "✅ /sync OK" || (echo "❌ /sync FAILED" && exit 1)

      - name: Smoke test — /db-mon?aspect=conn
        run: |
          STATUS=$(curl -sf -o /dev/null -w "%{http_code}" \
            "http://localhost:8000/db-mon?aspect=conn")
          [ "$STATUS" = "200" ] \
            && echo "✅ DB conn OK" \
            || (echo "❌ DB conn FAILED: $STATUS" && exit 1)

      - name: Smoke test — /data validation (expect 400)
        run: |
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" \
            "http://localhost:8000/data?sort_field=invalid&sort_order=ASC")
          [ "$STATUS" = "400" ] \
            && echo "✅ Validation 400 OK" \
            || (echo "❌ Unexpected: $STATUS" && exit 1)

      # ── s) Print summary ───────────────────────────────────────────────────
      - name: Print stack summary
        run: |
          echo ""
          echo "============================================"
          echo " Deployment Complete"
          echo "============================================"
          echo ""
          echo "APP CLUSTER (kind-rickandmorty):"
          echo "  API:          http://localhost:8000"
          echo "  Prometheus:   http://localhost:30909"
          echo "  Alertmanager: http://localhost:30980"
          echo ""
          echo "MONITORING CLUSTER (kind-monitoring):"
          echo "  Grafana:      http://localhost:3000  (admin/admin)"
          echo "  Alerta:       http://localhost:4000"
          echo ""
          echo "Federation:"
          echo "  Monitoring Prometheus scrapes app Prometheus"
          echo "  at ${{ env.HOST_IP }}:30909 via Docker bridge"
          echo ""

      # ── Diagnostics on failure ─────────────────────────────────────────────
      - name: Dump logs on failure
        if: failure()
        run: |
          echo "=== APP CLUSTER ==="
          kubectl config use-context kind-rickandmorty
          kubectl get pods -n ${{ env.K8S_NAMESPACE }}
          kubectl get pods -n monitoring
          kubectl logs -l app.kubernetes.io/name=rickandmorty-api \
            -n ${{ env.K8S_NAMESPACE }} --tail=100 || true
          kubectl logs -l app.kubernetes.io/name=prometheus \
            -n monitoring --tail=50 || true

          echo "=== MONITORING CLUSTER ==="
          kubectl config use-context kind-monitoring
          kubectl get pods -n monitoring
          kubectl logs -l app.kubernetes.io/name=prometheus \
            -n monitoring --tail=100 || true
          kubectl logs -l app.kubernetes.io/name=grafana \
            -n monitoring --tail=50 || true