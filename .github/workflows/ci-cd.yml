name: CI/CD Pipeline

on:
  push:
    branches: [main, master, develop]
  pull_request:
    branches: [main, master]

env:
  REGISTRY: docker.io
  IMAGE_NAME: ${{ secrets.DOCKERHUB_USERNAME }}/rickandmorty-api
  APP_DIR: .
  # Kubernetes namespace used in every kubectl/helm command
  K8S_NAMESPACE: rickandmorty

jobs:
  # ─────────────────────────────────────────────
  # 1. Code Quality
  # ─────────────────────────────────────────────
  lint-and-security:
    name: Lint & Security Scan
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python
        run: uv python install

      - name: Install dependencies
        working-directory: ${{ env.APP_DIR }}
        run: uv sync --all-extras

      # Ruff covers both linting and formatting checks
      - name: Lint with Ruff
        working-directory: ${{ env.APP_DIR }}
        run: uv run ruff check . --output-format=github

      - name: Format check with Ruff
        working-directory: ${{ env.APP_DIR }}
        run: uv run ruff format --check .

      # Static type checking
      - name: Type check with mypy
        working-directory: ${{ env.APP_DIR }}
        run: uv run mypy main.py --ignore-missing-imports
        continue-on-error: true   # advisory; flip to false to enforce

      # Security: dependency vulnerabilities
      - name: Security scan with pip-audit
        working-directory: ${{ env.APP_DIR }}
        run: uv run pip-audit

      # Security: source-code patterns (secrets, hardcoded creds, etc.)
      - name: Security scan with Bandit
        working-directory: ${{ env.APP_DIR }}
        run: uv run bandit -r main.py -c pyproject.toml -ll

  # ─────────────────────────────────────────────
  # 2. Unit Tests
  # ─────────────────────────────────────────────
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint-and-security

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Set up Python
        run: uv python install

      - name: Install dependencies
        working-directory: ${{ env.APP_DIR }}
        run: uv sync --all-extras

      # Provide dummy config/secret files so the module-level argparse
      # does not fail during import inside the test suite.
      - name: Create dummy config files
        run: |
          mkdir -p tmp
          echo '{"user":"test","password":"test","host":"localhost","dbname":"testdb"}' > tmp/secrets.json
          printf 'log_level: INFO\n' > tmp/config.yaml

      - name: Run unit tests
        working-directory: ${{ env.APP_DIR }}
        env:
          CONFIG_PATH: ../tmp/config.yaml
          SECRET_PATH: ../tmp/secrets.json
        run: |
          uv run pytest tests/unit/ \
            --tb=short \
            -v \
            --cov=main \
            --cov-report=xml \
            --cov-report=term-missing

      - name: Upload coverage report
        uses: codecov/codecov-action@v4
        if: always()
        with:
          files: ${{ env.APP_DIR }}/coverage.xml
          fail_ci_if_error: false

  # ─────────────────────────────────────────────
  # 3. Build & Test Docker Image
  # ─────────────────────────────────────────────
  build-and-test:
    name: Build & Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: testuser
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image (local, no push)
        uses: docker/build-push-action@v5
        with:
          context: ${{ env.APP_DIR }}
          push: false
          load: true
          tags: ${{ env.IMAGE_NAME }}:test
          cache-from: type=gha
          cache-to: type=gha,mode=max

      # ── Scan the built image for OS/package CVEs ──
      - name: Scan image with Trivy
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.IMAGE_NAME }}:test
          format: table
          exit-code: 1            # fail on CRITICAL vulnerabilities
          severity: CRITICAL
          ignore-unfixed: true

      # ── Spin up the app container for integration tests ──
      - name: Create runtime config files
        run: |
          mkdir -p tmp
          cat > tmp/secrets.json <<'EOF'
          {"user":"testuser","password":"testpass","host":"localhost","dbname":"rickandmorty"}
          EOF
          printf 'log_level: INFO\n' > tmp/config.yaml

      - name: Run application container
        run: |
          docker run -d \
            --name app \
            --network host \
            -v ${{ github.workspace }}/tmp:/tmp/appconfig:ro \
            ${{ env.IMAGE_NAME }}:test \
            --config /tmp/appconfig/config.yaml \
            --secret /tmp/appconfig/secrets.json

      - name: Wait for application to be ready
        run: |
          for i in $(seq 1 30); do
            if curl -sf http://localhost:8000/docs > /dev/null 2>&1; then
              echo "App is up!"
              break
            fi
            echo "Waiting... attempt $i"
            sleep 3
          done

      - name: Install uv (for integration test runner)
        uses: astral-sh/setup-uv@v4

      - name: Set up Python
        run: uv python install

      - name: Install test dependencies
        working-directory: ${{ env.APP_DIR }}
        run: uv sync --all-extras

      - name: Run integration tests
        working-directory: ${{ env.APP_DIR }}
        env:
          API_BASE_URL: http://localhost:8000
          DB_HOST: localhost
          DB_USER: testuser
          DB_PASSWORD: testpass
          DB_NAME: rickandmorty
        run: |
          uv run pytest tests/integration/ \
            --tb=short \
            -v

      - name: Dump container logs on failure
        if: failure()
        run: docker logs app

  # ─────────────────────────────────────────────
  # 4. Push to Docker Hub
  # ─────────────────────────────────────────────
  push-image:
    name: Push to Docker Hub
    runs-on: ubuntu-latest
    needs: build-and-test
    # Only push on commits to the default branch (not on PRs)
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')

    steps:
      #- name: Set current date as environment variable
      #  run: echo "NOW=$(date +'%Y-%m-%d')" >> $GITHUB_ENV

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Extract Docker metadata (tags & labels)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.IMAGE_NAME }}
          tags: type=raw,value=build-{{date 'YYYYMMDD-HHmm'}}-{{sha}}
          #tags: |
          #  type=sha,prefix=sha-
          #  type=ref,event=branch
          #  type=raw,value=build-${{ env.NOW }},enable=${{ github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' }}

      - name: Build and push image
        uses: docker/build-push-action@v5
        with:
          context: ${{ env.APP_DIR }}
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64,linux/arm64

# ───────────────────────────────────────────────────────────────────────────
  # 5. Deploy to KinD
  #
  # Why SecretStore and ExternalSecret are applied via kubectl and NOT via Helm:
  # Helm validates all chart resources against the live API server before apply.
  # If the ESO CRDs are not yet registered, Helm fails with:
  #   "no matches for kind ExternalSecret in version external-secrets.io/v1beta1"
  # Solution: keep the Helm chart free of ESO custom resources; apply them
  # with kubectl AFTER explicitly confirming the CRDs are Established.
  #
  # Flow:
  #   a) Write KinD config file and create cluster
  #   b) Build and load app image into KinD
  #   c) Install cert-manager (ESO hard dependency)
  #   d) Install ESO via Helm + wait for CRDs to be Established
  #   e) Create namespace + push raw secret
  #   f) Apply SecretStore and ExternalSecret via kubectl
  #   g) Deploy PostgreSQL
  #   h) Helm install app chart (no ESO kinds — safe to validate)
  #   i) Wait for ESO sync + Deployment rollout
  #   j) Smoke tests + integration tests
  # ───────────────────────────────────────────────────────────────────────────
  deploy-kind:
    name: Deploy to KinD
    runs-on: ubuntu-latest
    needs: push-image
    if: >-
      github.event_name == 'push' && 
      (
        github.ref == 'refs/heads/main' || 
        github.ref == 'refs/heads/master' || 
        startsWith(github.ref, 'refs/heads/feature_')
      )

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # ── a) KinD cluster ───────────────────────────────────────────────────
      - name: Write KinD cluster config
        run: |
          cat > /tmp/kind-config.yaml << 'EOF'
          kind: Cluster
          apiVersion: kind.x-k8s.io/v1alpha4
          nodes:
            - role: control-plane
              extraPortMappings:
                - containerPort: 30800
                  hostPort: 8000
                  protocol: TCP
          EOF

      - name: Create KinD cluster
        uses: helm/kind-action@v1.10.0
        with:
          cluster_name: rickandmorty
          config: /tmp/kind-config.yaml

      # ── b) Build and load app image ───────────────────────────────────────
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build image for KinD (amd64 only, local load)
        uses: docker/build-push-action@v5
        with:
          context: ${{ env.APP_DIR }}
          push: false
          load: true
          tags: ${{ env.IMAGE_NAME }}:kind
          cache-from: type=gha

      - name: Load image into KinD
        run: kind load docker-image ${{ env.IMAGE_NAME }}:kind --name rickandmorty

      # ── c) cert-manager ───────────────────────────────────────────────────
      - name: Install cert-manager
        run: |
          helm repo add jetstack https://charts.jetstack.io --force-update
          helm upgrade --install cert-manager jetstack/cert-manager \
            --namespace cert-manager \
            --create-namespace \
            --set installCRDs=true \
            --wait \
            --timeout 3m

      # ── d) ESO + wait for CRDs ────────────────────────────────────────────
      # Install ESO into the same namespace as the app so its ServiceAccount
      # exists in rickandmorty and can read secrets there without any
      # cross-namespace RBAC. CRDs are cluster-scoped so namespace doesn't
      # affect their availability.
      - name: Create namespace
        run: kubectl create namespace ${{ env.K8S_NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Install External Secrets Operator
        run: |
          helm repo add external-secrets https://charts.external-secrets.io --force-update
          helm upgrade --install external-secrets external-secrets/external-secrets \
            --namespace ${{ env.K8S_NAMESPACE }} \
            --set installCRDs=true \
            --wait \
            --timeout 3m

      # kubectl wait --for=condition=Established confirms the CRD object exists,
      # but the API server's REST discovery cache can still lag behind.
      # The only reliable gate is to poll kubectl api-resources until the
      # SecretStore kind actually appears in the discovery list.
      - name: Wait for ESO CRDs to be Established and discoverable
        run: |
          for crd in \
            externalsecrets.external-secrets.io \
            secretstores.external-secrets.io \
            clustersecretstores.external-secrets.io; do
            echo "Waiting for CRD object: $crd"
            kubectl wait --for=condition=Established crd/$crd --timeout=60s
          done

          echo "Polling until SecretStore appears in API discovery..."
          echo $(kubectl api-resources --api-group=external-secrets.io)
          for i in $(seq 1 30); do
            if kubectl api-resources --api-group=external-secrets.io 2>/dev/null | grep -q "secretstores.*v1"; then
              echo "SecretStore is discoverable after $i attempts."
              break
            fi
            echo "Attempt $i/30 — not yet discoverable, retrying in 5s..."
            sleep 5
          done

          if ! kubectl api-resources --api-group=external-secrets.io 2>/dev/null | grep -q "secretstores.*v1"; then
            echo "ERROR: SecretStore never became discoverable."
            exit 1
          fi

      # ── e) Push raw secret ────────────────────────────────────────────────
      - name: Push db secret to cluster
        env:
          DB_SECRET_JSON: ${{ secrets.RICKANDMORTY_DB_SECRET }}
        run: |
          kubectl create secret generic rickandmorty-db-raw \
            --from-literal=secrets.json="${DB_SECRET_JSON}" \
            --namespace ${{ env.K8S_NAMESPACE }} \
            --dry-run=client -o yaml | kubectl apply -f -

      # ── f) Apply ESO resources via kubectl (NOT via Helm) ─────────────────
      # ESO ServiceAccount is now in rickandmorty — no cross-namespace RBAC needed.
      - name: Apply SecretStore
        run: |
          kubectl apply -f - <<'EOF'
          apiVersion: external-secrets.io/v1
          kind: SecretStore
          metadata:
            name: rickandmorty-secret-store
            namespace: rickandmorty
          spec:
            provider:
              kubernetes:
                remoteNamespace: rickandmorty
                server:
                  caProvider:
                    type: ConfigMap
                    name: kube-root-ca.crt
                    key: ca.crt
                auth:
                  serviceAccount:
                    name: external-secrets
                    namespace: rickandmorty
          EOF

      - name: Apply ExternalSecret
        run: |
          kubectl apply -f - <<'EOF'
          apiVersion: external-secrets.io/v1
          kind: ExternalSecret
          metadata:
            name: rickandmorty-api-db
            namespace: rickandmorty
          spec:
            refreshInterval: 1h
            secretStoreRef:
              kind: SecretStore
              name: rickandmorty-secret-store
            target:
              name: rickandmorty-db-secret
              creationPolicy: Owner
              template:
                type: Opaque
                data:
                  secrets.json: "{{ .secretsJson }}"
            data:
              - secretKey: secretsJson
                remoteRef:
                  key: rickandmorty-db-raw
                  property: secrets.json
          EOF

      # ── g) PostgreSQL ─────────────────────────────────────────────────────
      - name: Deploy PostgreSQL
        run: |
          helm repo add bitnami https://charts.bitnami.com/bitnami --force-update
          helm upgrade --install postgres bitnami/postgresql \
            --namespace ${{ env.K8S_NAMESPACE }} \
            --set auth.username=rickandmorty \
            --set auth.password=rickandmorty \
            --set auth.database=rickandmorty \
            --wait \
            --timeout 3m

      # ── h) Helm install app chart (ESO kinds removed — safe to validate) ──
      - name: Helm install rickandmorty-api
        run: |
          helm upgrade --install rickandmorty-api ./helm/rickandmorty-api \
            --namespace ${{ env.K8S_NAMESPACE }} \
            --set image.repository=${{ env.IMAGE_NAME }} \
            --set image.tag=kind \
            --set image.pullPolicy=Never \
            --set service.type=NodePort \
            --set service.nodePort=30800 \
            --set config.log_level=INFO \
            --set externalSecret.targetSecretName=rickandmorty-db-secret \
            --wait \
            --timeout 5m

      # ── i) Wait for ESO sync + rollout ───────────────────────────────────
      - name: Wait for ExternalSecret to sync
        run: |
          for i in $(seq 1 24); do
            STATUS=$(kubectl get externalsecret rickandmorty-api-db \
              -n ${{ env.K8S_NAMESPACE }} \
              -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "")
            if [ "$STATUS" = "True" ]; then
              echo "ExternalSecret synced."
              break
            fi
            echo "Attempt $i/24 — status: '${STATUS}'. Retrying in 5s..."
            sleep 5
          done
          kubectl get externalsecret -n ${{ env.K8S_NAMESPACE }}

      - name: Wait for Deployment rollout
        run: |
          kubectl rollout status deployment/rickandmorty-api \
            --namespace ${{ env.K8S_NAMESPACE }} \
            --timeout=3m

      # ── j) Smoke tests ────────────────────────────────────────────────────
      - name: Smoke test — /sync reachable
        run: |
          curl -sf http://localhost:8000/sync/?source_url=rickandmortyapi.com&resource=character > /dev/null && \
            echo "✅ /sync OK" || (echo "❌ /sync FAILED" && exit 1)

      - name: Smoke test — /db-mon?aspect=conn
        run: |
          STATUS=$(curl -sf -o /dev/null -w "%{http_code}" "http://localhost:8000/db-mon?aspect=conn")
          [ "$STATUS" = "200" ] && echo "✅ DB conn OK" || (echo "❌ DB conn FAILED: $STATUS" && exit 1)

      - name: Smoke test — /data validation (expect 400)
        run: |
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" \
            "http://localhost:8000/data?sort_field=invalid&sort_order=ASC")
          [ "$STATUS" = "400" ] && echo "✅ Validation 400 OK" || (echo "❌ Unexpected: $STATUS" && exit 1)

      # ── Diagnostics on failure ────────────────────────────────────────────
      - name: Dump logs on failure
        if: failure()
        run: |
          echo "=== Pods ==="
          kubectl get pods -n ${{ env.K8S_NAMESPACE }}

          echo "=== ExternalSecret ==="
          kubectl describe externalsecret -n ${{ env.K8S_NAMESPACE }} || true

          echo "=== App logs ==="
          kubectl logs -l app.kubernetes.io/name=rickandmorty-api \
            -n ${{ env.K8S_NAMESPACE }} --tail=100 || true

          echo "=== ESO logs ==="
          kubectl logs -l app.kubernetes.io/name=external-secrets \
            -n ${{ env.K8S_NAMESPACE }} --tail=50 || true

# ───────────────────────────────────────────────────────────────────────────
  # 6. Deploy Monitoring Stack (Prometheus + Grafana + Alerta)
  #
  # Cross-cluster monitoring architecture:
  #   - App Cluster (rickandmorty): kube-prometheus-stack collects metrics from rickandmorty-api
  #   - Monitoring Cluster (monitoring): Prometheus federates from app cluster + Grafana + Alerta
  #
  # This job:
  #   a) Installs kube-prometheus-stack in APP cluster (rickandmorty) - collects app metrics
  #   b) Creates second KinD cluster (monitoring) for centralized monitoring
  #   c) Configures Prometheus federation - monitoring cluster scrapes from app cluster
  #   d) Installs Grafana + Alerta in monitoring cluster
  #   e) Deploys ServiceMonitors in APP cluster (rickandmorty)
  #   f) Creates alerting rules
  #   g) Creates Grafana dashboards
  # ───────────────────────────────────────────────────────────────────────────
  monitoring-stack:
    name: Deploy Monitoring Stack
    runs-on: ubuntu-latest
    needs: deploy-kind
    if: >-
      github.event_name == 'push' && 
      (
        github.ref == 'refs/heads/main' || 
        github.ref == 'refs/heads/master' || 
        startsWith(github.ref, 'refs/heads/feature_')
      )

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # ── a) Install kube-prometheus-stack in APP cluster (rickandmorty) ─────
      - name: Add Prometheus community repos
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts --force-update
          helm repo update

      - name: Install kube-prometheus-stack in app cluster (rickandmorty)
        run: |
          # Point to app cluster (rickandmorty)
          kubectl config use-context kind-rickandmorty

          helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --create-namespace \
            --set prometheus.prometheusSpec.retention=7d \
            --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resourcesrequests.storage=8Gi \
            --set grafana.enabled=false \
            --set alertmanager.enabled=true \
            --set alertmanager.service.type=NodePort \
            --set alertmanager.service.nodePort=30980 \
            --set prometheus.service.type=NodePort \
            --set prometheus.service.nodePort=30909 \
            --wait \
            --timeout 5m

      # ── b) Create ServiceMonitor for rickandmorty-api in APP cluster ─────
      - name: Apply ServiceMonitor in app cluster
        run: |
          kubectl config use-context kind-rickandmorty
          kubectl apply -f ./monitoring/prometheus/servicemonitor.yaml

      # ── c) Create PrometheusRule in APP cluster ───────────────────────────
      - name: Apply Prometheus alerting rules in app cluster
        run: |
          kubectl config use-context kind-rickandmorty
          kubectl apply -f ./monitoring/prometheus/rules.yaml

      # ── d) Create second KinD cluster for monitoring ───────────────────────
      - name: Write monitoring KinD cluster config
        run: |
          cat > /tmp/kind-config-monitoring.yaml << 'EOF'
          kind: Cluster
          apiVersion: kind.x-k8s.io/v1alpha4
          nodes:
            - role: control-plane
              extraPortMappings:
                - containerPort: 30300
                  hostPort: 3000
                  protocol: TCP
                - containerPort: 30400
                  hostPort: 4000
                  protocol: TCP
                - containerPort: 30980
                  hostPort: 9980
                  protocol: TCP
          EOF

      - name: Create monitoring KinD cluster
        uses: helm/kind-action@v1.10.0
        with:
          cluster_name: monitoring
          config: /tmp/kind-config-monitoring.yaml

      # ── e) Install monitoring components in monitoring cluster ─────────────
      - name: Install Grafana + Alerta in monitoring cluster
        run: |
          kubectl config use-context kind-monitoring

          # Install kube-prometheus-stack (just for Grafana, not Prometheus)
          helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --create-namespace \
            --set prometheus.enabled=false \
            --set prometheusOperator.enabled=true \
            --set grafana.enabled=true \
            --set grafana.service.type=NodePort \
            --set grafana.service.nodePort=30300 \
            --set grafana.adminPassword=admin \
            --set alertmanager.enabled=true \
            --set alertmanager.service.type=NodePort \
            --set alertmanager.service.nodePort=30980 \
            --wait \
            --timeout 5m

          # Install Alerta
          helm repo add alerta https://alerta.github.io/helm-charts --force-update
          helm upgrade --install alerta alerta/alerta \
            --namespace monitoring \
            --set persistence.enabled=true \
            --set persistence.size=1Gi \
            --set service.type=NodePort \
            --set service.nodePort=30400 \
            --set config.authEnabled=false \
            --set config.demoMode=true \
            --wait \
            --timeout 3m

      # ── f) Configure Prometheus federation ─────────────────────────────────
      - name: Configure Prometheus federation
        run: |
          kubectl config use-context kind-monitoring

          # Apply Prometheus with federation config to scrape from app cluster
          kubectl apply -f - <<'EOF'
          apiVersion: monitoring.coreos.com/v1
          kind: Prometheus
          metadata:
            name: federation
            namespace: monitoring
          spec:
            replicas: 1
            retention: 7d
            storage:
              volumeClaimTemplate:
                spec:
                  resources:
                    requests:
                      storage: 8Gi
            serviceAccountName: prometheus
            serviceMonitorSelector:
              matchLabels:
                federate: "true"
            podMonitorSelector: {}
            ruleSelector: {}
            alerting:
              alertmanagers:
                - namespace: monitoring
                  name: prometheus-alertmanager
                  port: web
            externalLabels:
              cluster: monitoring
            remoteWrite:
              - url: "http://prometheus.monitoring.svc.cluster.local:9090/api/v1/write"
            # Federation config - scrape from app cluster's Prometheus
            federation:
              endpoints:
                - port: web
                  interval: 30s
          EOF

      # Create a Service to expose app cluster Prometheus for federation
      - name: Expose app cluster Prometheus for federation
        run: |
          # Use hostNetwork to access the app cluster's Prometheus from monitoring cluster
          # Since they're in different clusters, we need to use the host ports
          echo "App cluster Prometheus accessible at: http://localhost:30909"

      # ── g) Add app cluster as federated source ─────────────────────────────
      - name: Add app cluster Prometheus as federated source
        run: |
          kubectl config use-context kind-monitoring

          # Create a ServiceMonitor that uses the app cluster's Prometheus endpoint
          # Since we're in CI, we configure the monitoring Prometheus to federate
          # by using the remoteWrite from app cluster or by configuring federation jobs
          # For KinD, we'll use a simpler approach - configure the monitoring Prometheus
          # to scrape from app cluster via NodePort
          kubectl apply -f - <<'EOF'
          apiVersion: monitoring.coreos.com/v1
          kind: ServiceMonitor
          metadata:
            name: federated-rickandmorty
            namespace: monitoring
            labels:
              release: prometheus
              federate: "true"
          spec:
            endpoints:
              - interval: 30s
                port: web
                path: /federate
                params:
                  'match[]':
                    - '{__name__=~".*"}'
            namespaceSelector: {}
            selector: {}
          EOF

      # ── h) Configure Alertmanager to send alerts to Alerta ───────────────
      - name: Configure Alertmanager for Alerta
        run: |
          kubectl config use-context kind-monitoring

          kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus-alertmanager -n monitoring --timeout=120s || true

          kubectl apply -f ./monitoring/prometheus/alertmanager-config.yaml

      # ── i) Create Grafana dashboards ConfigMap ───────────────────────────
      - name: Create Grafana dashboards ConfigMap
        run: |
          kubectl config use-context kind-monitoring

          # Apply dashboard ConfigMaps from external files
          # Dashboard 1: API Overview
          kubectl create configmap rickandmorty-api-dashboard \
            --from-file=rickandmorty-api-dashboard.json=./monitoring/grafana/dashboards/rickandmorty-api-dashboard.json \
            --namespace=monitoring \
            --labels=grafana_dashboard=1 \
            --dry-run=client -o yaml | kubectl apply -f -

          # Dashboard 2: Cluster Overview
          kubectl create configmap rickandmorty-cluster-dashboard \
            --from-file=rickandmorty-cluster-dashboard.json=./monitoring/grafana/dashboards/rickandmorty-cluster-dashboard.json \
            --namespace=monitoring \
            --labels=grafana_dashboard=1 \
            --dry-run=client -o yaml | kubectl apply -f -

      # ── j) Verify monitoring stack ───────────────────────────────────────
      - name: Verify app cluster monitoring
        run: |
          kubectl config use-context kind-rickandmorty
          echo "=== App Cluster (rickandmorty) Monitoring ==="
          kubectl get pods -n monitoring
          kubectl get svc -n monitoring

      - name: Verify monitoring cluster
        run: |
          kubectl config use-context kind-monitoring
          echo "=== Monitoring Cluster ==="
          kubectl get pods -n monitoring
          kubectl get svc -n monitoring

      # ── k) Print monitoring URLs ──────────────────────────────────────────
      - name: Print monitoring URLs
        run: |
          echo ""
          echo "=========================================="
          echo "Monitoring Stack Deployed Successfully!"
          echo "=========================================="
          echo ""
          echo "APP CLUSTER (rickandmorty):"
          echo "  Prometheus: http://localhost:30909"
          echo "  Alertmanager: http://localhost:30980"
          echo ""
          echo "MONITORING CLUSTER:"
          echo "  Grafana: http://localhost:3000 (admin/admin)"
          echo "  Alerta: http://localhost:4040 (demo mode)"
          echo ""
          echo "Cross-cluster setup:"
          echo "  - App cluster has Prometheus collecting rickandmorty-api metrics"
          echo "  - Monitoring cluster has Grafana + Alerta"
          echo "  - Configure federation to federate metrics between clusters"
          echo ""
          echo "Dashboards created:"
          echo "  - RickAndMorty API Overview"
          echo "  - RickAndMorty Cluster Overview"
          echo ""
          echo "Alerts configured:"
          echo "  - RickAndMortyAPIHighErrorRate"
          echo "  - RickAndMortyAPILatencyHigh"
          echo "  - RickAndMortyAPIDown"
          echo "  - RickAndMortyDatabaseConnectionHigh"
          echo "  - RickAndMortySyncJobFailing"
          echo ""
